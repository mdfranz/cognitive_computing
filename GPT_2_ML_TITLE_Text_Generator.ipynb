{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 ML TITLE Text-Generator",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Retrain GPT-2 and Generate Title\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "!pip install -q arxivscraper\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import arxivscraper as ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "89f72273-2824-4b7a-d816-b6af13a1d3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Mar 19 20:33:18 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dr5-6B2exYX",
        "colab_type": "text"
      },
      "source": [
        "# Scrape data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXDXwEj6e4th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# scraper for arxiv physics\n",
        "scraper = ax.Scraper(category='physics', date_from='2019-05-01',\n",
        "                     date_until='2019-07-03', t=10,\n",
        "                     filters={'categories':['quant-ph']})\n",
        "\n",
        "# scraper for arxiv q-bio\n",
        "scraper = ax.Scraper(category='q-bio', date_from='2016-08-01',\n",
        "                     date_until='2019-07-01', t=10, \n",
        "                     filters={'categories':['q-bio.GN', 'q-bio.NC']})\n",
        "'''\n",
        "\n",
        "# scraper for arxiv stat.ml\n",
        "scraper = ax.Scraper(category='stat', date_from='2019-08-01',\n",
        "                     date_until='2020-03-01', t=10, \n",
        "                     filters={'categories':['stat.ml'],'abstract':['learning']})\n",
        "output = scraper.scrape()\n",
        "\n",
        "\n",
        "# cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
        "titles = [' '.join(o['title'].split()) for o in output]\n",
        "np.savetxt('titles.csv', np.array(titles), fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Fine-Tune GPT-2\n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "b24b8c9e-3011-4720-b1f2-7e3c99c58ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model_name = \"124M\"\n",
        "gpt2.download_gpt2(model_name=model_name)   "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 757Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 156Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 782Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:05, 93.8Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 607Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 209Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 310Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "0b5584ae-7d0a-490b-e471-c1a9972e754f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"titles.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5oy3JaEiCdX",
        "colab_type": "code",
        "outputId": "f9384825-b855-4789-ea28-89f2275dec31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,\n",
        "              'titles.csv',\n",
        "              model_name='124M',\n",
        "              steps=200,\n",
        "              run_name='run2',\n",
        "              save_every=100,\n",
        "              sample_every=25)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 57.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 301567 tokens\n",
            "Training...\n",
            "[1 | 8.35] loss=2.33 avg=2.33\n",
            "[2 | 9.24] loss=2.52 avg=2.42\n",
            "[3 | 10.12] loss=2.36 avg=2.40\n",
            "[4 | 11.01] loss=2.32 avg=2.38\n",
            "[5 | 11.89] loss=2.16 avg=2.33\n",
            "[6 | 12.78] loss=2.16 avg=2.30\n",
            "[7 | 13.66] loss=2.38 avg=2.32\n",
            "[8 | 14.54] loss=2.37 avg=2.32\n",
            "[9 | 15.43] loss=2.40 avg=2.33\n",
            "[10 | 16.31] loss=2.14 avg=2.31\n",
            "[11 | 17.19] loss=2.02 avg=2.28\n",
            "[12 | 18.08] loss=2.16 avg=2.27\n",
            "[13 | 18.96] loss=2.17 avg=2.26\n",
            "[14 | 19.84] loss=1.87 avg=2.23\n",
            "[15 | 20.72] loss=2.23 avg=2.23\n",
            "[16 | 21.60] loss=2.18 avg=2.23\n",
            "[17 | 22.49] loss=2.05 avg=2.22\n",
            "[18 | 23.37] loss=2.08 avg=2.21\n",
            "[19 | 24.25] loss=2.07 avg=2.20\n",
            "[20 | 25.13] loss=2.16 avg=2.20\n",
            "[21 | 26.01] loss=2.10 avg=2.19\n",
            "[22 | 26.90] loss=2.18 avg=2.19\n",
            "[23 | 27.78] loss=2.03 avg=2.19\n",
            "[24 | 28.67] loss=2.09 avg=2.18\n",
            "[25 | 29.55] loss=2.03 avg=2.17\n",
            "======== SAMPLE 1 ========\n",
            ">startoftext|>a generalization with finite-state graphs for training neural networks: an example from the greek translation problems with nonzero error bounds<|endoftext|>\n",
            "<|startoftext|>learning probabilistic model for a human-centered data distribution<|endoftext|>\n",
            "<|startoftext|>the neural code for multi-shot games<|endoftext|>\n",
            "<|startoftext|>learning robust classification models using the human-centered method<|endoftext|>\n",
            "<|startoftext|>learning a model from multiple-valued data for supervised visual recognition<|endoftext|>\n",
            "<|startoftext|>an overview of multi-task classification with deep neural networks based on a mixture of offline training<|endoftext|>\n",
            "<|startoftext|>deep recurrent neural networks for multi-task classification<|endoftext|>\n",
            "<|startoftext|>a multi-task model based on multi-state stochastic distribution inference<|endoftext|>\n",
            "<|startoftext|>multi-agent and multiple-agent deep recurrent neural network for deep reinforcement learning<|endoftext|>\n",
            "<|startoftext|>bounceback-based non-recurrent neural networks in sparse and sparsely sampled neural network-based generative model estimation<|endoftext|>\n",
            "<|startoftext|>towards an optimal learning rate<|endoftext|>\n",
            "<|startoftext|>towards a deep gradient of action potential for action signal generation<|endoftext|>\n",
            "<|startoftext|>on-demand action prediction as a feature of network analysis<|endoftext|>\n",
            "<|startoftext|>autoelectrode and non-autoelectrode robot training from data: an experiment<|endoftext|>\n",
            "<|startoftext|>a generalized learning rate and a distribution with different distributions<|endoftext|>\n",
            "<|startoftext|>towards automated neural neural network training in multivariate time series classification<|endoftext|>\n",
            "<|startoftext|>multivariate linear regression estimation: on-demand detection for multi-dimensional data<|endoftext|>\n",
            "<|startoftext|>towards the optimal solution for autoencoders training with a low power of knowledge in neural network autoencoder with deep autoencoder<|endoftext|>\n",
            "<|startoftext|>automatic and semi-automatic learning of the sequence for the classification of images<|endoftext|>\n",
            "<|startoftext|>probabilistic and generalized linear models (e.g. for reinforcement learning) for adversarial optimization<|endoftext|>\n",
            "<|startoftext|>predicting the next step of a neural-net gradient descent for deep neural network training<|endoftext|>\n",
            "<|startoftext|>detonation with stochastic methods: an application to deep gradient descent<|endoftext|>\n",
            "<|startoftext|>a non-parametric approach for the evaluation of computational complexity<|endoftext|>\n",
            "<|startoftext|>multilamined gaussian processes: a novel non-parametric approach to estimating the k-dashed gaussian of a gaussian distribution<|endoftext|>\n",
            "<|startoftext|>training a neural network for image data using image-based visualization of a model in the real world <|endoftext|>\n",
            "<|startoftext|>a generalization of the probabilistic and probabilistic-policy learning model that exploits robustness, stability, and generality of policy<|endoftext|>\n",
            "<|startoftext|>exploiting the variance of a latent variable with a probability distribution<|endoftext|>\n",
            "<|startoftext|>using reinforcement learning to build an understanding algorithm for non-cognitive skills<|endoftext|>\n",
            "<|startoftext|>on the problem of the human and artificial self-learning algorithms: a case study from an artificial intelligence conference conference<|endoftext|>\n",
            "<|startoftext|>adaptive gaussian process estimation for deep reinforcement learning of a system with unknown neural network data<|endoftext|>\n",
            "<|startoftext|>a new approach in computer vision for neural network training to model faces<|endoftext|>\n",
            "<|startoftext|>towards automated neural network training of a neural network from local data in the real world and a case study based on machine learning in a large-scale neural network<|endoftext|>\n",
            "<|startoftext\n",
            "\n",
            "[26 | 48.74] loss=2.11 avg=2.17\n",
            "[27 | 49.63] loss=2.29 avg=2.18\n",
            "[28 | 50.51] loss=1.97 avg=2.17\n",
            "[29 | 51.39] loss=2.02 avg=2.16\n",
            "[30 | 52.28] loss=2.21 avg=2.16\n",
            "[31 | 53.17] loss=1.83 avg=2.15\n",
            "[32 | 54.06] loss=2.01 avg=2.15\n",
            "[33 | 54.94] loss=2.03 avg=2.14\n",
            "[34 | 55.82] loss=2.02 avg=2.14\n",
            "[35 | 56.70] loss=1.85 avg=2.13\n",
            "[36 | 57.58] loss=1.99 avg=2.12\n",
            "[37 | 58.46] loss=2.10 avg=2.12\n",
            "[38 | 59.34] loss=2.00 avg=2.12\n",
            "[39 | 60.23] loss=2.07 avg=2.12\n",
            "[40 | 61.11] loss=2.16 avg=2.12\n",
            "[41 | 61.99] loss=1.90 avg=2.11\n",
            "[42 | 62.87] loss=2.08 avg=2.11\n",
            "[43 | 63.75] loss=1.85 avg=2.10\n",
            "[44 | 64.64] loss=2.09 avg=2.10\n",
            "[45 | 65.52] loss=1.99 avg=2.10\n",
            "[46 | 66.40] loss=2.04 avg=2.10\n",
            "[47 | 67.28] loss=1.89 avg=2.09\n",
            "[48 | 68.16] loss=2.14 avg=2.09\n",
            "[49 | 69.04] loss=1.87 avg=2.09\n",
            "[50 | 69.93] loss=2.21 avg=2.09\n",
            "======== SAMPLE 1 ========\n",
            " using static analysis of complex data . Methods, Applications and Interpretation : a review . Journal of Experimental Analysis: Neural Networks in the Social Sciences. 2012 . 5: 857 - 918 . doi: 10.1007/s11911-012-4167-9 PubMed] Loomis J, Johnson M.\n",
            "\n",
            "et al. deep learning for large-scale data-driven optimization . arXiv:1512.90316\n",
            "\n",
            "et al. learning patterns from sparse data: an applied and deep learning framework for predicting earthquakes using multiple earthquake observations . mrp(es)jr(es)=mrtp.2013i0122#.UJq9WXgfRk-qx.pbf Google Scholar\n",
            "\n",
            "et al. learning data for high confidence by using continuous learning to predict large samples . lme-med<--log-mail-med-classification-with-constraints<--endoftext-->\n",
            "\n",
            "et al. a multi-stage approach to identifying deep neural networks with sparse training<--endoftext-->\n",
            "\n",
            "et al. deep reinforcement learning on a continuous reinforcement network: an experimental analysis of the role of network features in the reinforcement learning process<--endoftext-->\n",
            "\n",
            "et al. a model-view-based approach to deep learning in small classes<--endoftext-->\n",
            "\n",
            "et al. learning a gaussian model for the sparse gaussian method<--endoftext-->\n",
            "\n",
            "et al. the cost of convergence of deep neural network training<--endoftext-->\n",
            "\n",
            "et al. recurrent neural networks and machine learning: the impact of neural network generative adversarial network on the accuracy of gaussian models<--endoftext-->\n",
            "\n",
            "et al. multiple prediction errors for deep neural network training<--endoftext-->\n",
            "\n",
            "et al. adversarial adversarial and non-adversarial attacks against deep reinforcement learning<--endoftext-->\n",
            "\n",
            "et al. deep hierarchical stochastic gradient descent models<--endoftext-->\n",
            "\n",
            "et al. multidirectional gradient descent of deep neural networks for the analysis of neural connectivity<--endoftext-->\n",
            "\n",
            "et al. generalization of data distribution by sparse-set deep neural networks<--endoftext-->\n",
            "\n",
            "et al. time series forecasting using a model of time on the internet<--endoftext-->\n",
            "\n",
            "et al. prediction with reinforcement learning<--endoftext-->\n",
            "\n",
            "et al. reinforcement learning with stochastic differential inequalities<--endoftext-->\n",
            "\n",
            "et al. reinforcement learning for automatic classification of non-stationary and stationary entities<--endoftext-->\n",
            "\n",
            "et al. robust adversarial attacks against graph convolutional networks with an uncertainty level of pf<--endoftext-->\n",
            "\n",
            "et al. recurrent neural networks to predict single photon emission tomographic image velocities<--endoftext-->\n",
            "\n",
            "et al. deep gaussian model for large nonconvex optimization<--endoftext-->\n",
            "\n",
            "et al. a comparison of adversarial attacks against deep learning with reinforcement learning in natural image prediction<--endoftext-->\n",
            "\n",
            "et al. time-series with gaussian mixture model for predictability of climate-related events<--endoftext-->\n",
            "\n",
            "et al. the generative adversarial and reinforcement learning problem<--endoftext-->\n",
            "\n",
            "et al. deep reinforcement learning for efficient reinforcement learning and its applications with stochastically large training and learning rates<--endoftext-->\n",
            "\n",
            "et al. generalized stochastic gradient descent models for the detection of large-scale data dependencies<--endoftext-->\n",
            "\n",
            "et al. large-scale learning of time-series classification schemes<--endoftext-->\n",
            "\n",
            "et al. the influence of adversarial attacks on deep reinforcement learning<--endoftext-->\n",
            "\n",
            "et al. a probit-based approach to the understanding of graph convolutional networks<--endoftext-->\n",
            "\n",
            "et al. gradient methods on graph convolutional neural network training<--endoftext-->\n",
            "\n",
            "et al. graph convolutional neural networks with stochastic differentiation model<--endoftext-->\n",
            "\n",
            "et al. robust adversarial attacks against deep reinforcement learning<--endoftext-->\n",
            "\n",
            "et al. reinforcement learning with nonconvex optimization<--endoftext-->\n",
            "\n",
            "et al. an empirical study and demonstration of deep reinforcement learning for fast and efficient reinforcement learning<--endoftext-->\n",
            "\n",
            "et al. nonconvex prediction for stochastic classification via reinforcement learning with no constraints<--endoftext-->\n",
            "\n",
            "et al. fast time-sparse estimation of sparse data in nonconvex optimization for graph convolutional neural network models<\n",
            "\n",
            "[51 | 86.77] loss=1.85 avg=2.09\n",
            "[52 | 87.66] loss=1.77 avg=2.08\n",
            "[53 | 88.54] loss=2.01 avg=2.08\n",
            "[54 | 89.42] loss=2.01 avg=2.07\n",
            "[55 | 90.30] loss=2.05 avg=2.07\n",
            "[56 | 91.18] loss=1.99 avg=2.07\n",
            "[57 | 92.07] loss=2.07 avg=2.07\n",
            "[58 | 92.96] loss=1.95 avg=2.07\n",
            "[59 | 93.84] loss=2.01 avg=2.07\n",
            "[60 | 94.72] loss=2.01 avg=2.07\n",
            "[61 | 95.60] loss=2.21 avg=2.07\n",
            "[62 | 96.49] loss=2.02 avg=2.07\n",
            "[63 | 97.37] loss=1.81 avg=2.06\n",
            "[64 | 98.25] loss=2.05 avg=2.06\n",
            "[65 | 99.13] loss=2.01 avg=2.06\n",
            "[66 | 100.01] loss=2.01 avg=2.06\n",
            "[67 | 100.90] loss=2.07 avg=2.06\n",
            "[68 | 101.78] loss=1.97 avg=2.06\n",
            "[69 | 102.66] loss=2.04 avg=2.06\n",
            "[70 | 103.54] loss=2.15 avg=2.06\n",
            "[71 | 104.43] loss=1.87 avg=2.06\n",
            "[72 | 105.31] loss=1.79 avg=2.05\n",
            "[73 | 106.19] loss=1.79 avg=2.05\n",
            "[74 | 107.07] loss=1.90 avg=2.04\n",
            "[75 | 107.96] loss=1.99 avg=2.04\n",
            "======== SAMPLE 1 ========\n",
            "ering text: a test of Bayesian optimization in data-driven deep learning<|endoftext|>\n",
            "<|startoftext|>tot: a graph-based classification tool for visual information retrieval<|endoftext|>\n",
            "<|startoftext|>adaptive regression in image classification: probabilistic deep neural networks<|endoftext|>\n",
            "<|startoftext|>generalised learning of m-approximate values of parameters in mri data<|endoftext|>\n",
            "<|startoftext|>data-driven training of graph neural networks<|endoftext|>\n",
            "<|startoftext|>generalization of latent matrix factorization<|endoftext|>\n",
            "<|startoftext|>on the quality of deep learning models<|endoftext|>\n",
            "<|startoftext|>model-agnostic data science training: a comparative study with deep learning<|endoftext|>\n",
            "<|startoftext|>on the influence of features in the classification rate of deep neural networks<|endoftext|>\n",
            "<|startoftext|>learning deep neural networks from input data by visual analysis<|endoftext|>\n",
            "<|startoftext|>deep neural networks: a new classifier to replace classic ones<|endoftext|>\n",
            "<|startoftext|>learning to recognize and react to naturalistic stimuli on face recognition data in speech and vision domain<|endoftext|>\n",
            "<|startoftext|>deep classification of human behavior in video surveillance systems<|endoftext|>\n",
            "<|startoftext|>inverse deep neural network with non-linear scaling and gaussian process<|endoftext|>\n",
            "<|startoftext|>towards learning continuous, non-linear distributions for gaussian processes<|endoftext|>\n",
            "<|startoftext|>model-agnostic regression of model-agnostic data using regression in machine learning<|endoftext|>\n",
            "<|startoftext|>improving the accuracy of deep neural networks<|endoftext|>\n",
            "<|startoftext|>deep learning for text analysis and identification<|endoftext|>\n",
            "<|startoftext|>reinforcement learning in visual perception: from visual perception to deep learning applications<|endoftext|>\n",
            "<|startoftext|>dear fellow, let's learn some deep neural networks for the problem of language recognition with speech-language-impaired users<|endoftext|>\n",
            "<|startoftext|>deep learning for speech recognition in visual perception<|endoftext|>\n",
            "<|startoftext|>distributed learning of deep neural networks<|endoftext|>\n",
            "<|startoftext|>deep neural network with unsupervised object detection in complex video games<|endoftext|>\n",
            "<|startoftext|>learning the flow of reinforcement learning<|endoftext|>\n",
            "<|startoftext|>deep learning for dynamic distribution clustering of a system with multiple discrete distributions<|endoftext|>\n",
            "<|startoftext|>learning from deep-labeled images with deep hypertext features<|endoftext|>\n",
            "<|startoftext|>learning multiple data sources from simple media based on deep learning<|endoftext|>\n",
            "<|startoftext|>data-driven machine learning from images to detect suspicious patterns<|endoftext|>\n",
            "<|startoftext|>anomaly-abated deep learning learning of high-performance deep neural networks<|endoftext|>\n",
            "<|startoftext|>on the accuracy of machine-learning-based hypergraphs<|endoftext|>\n",
            "<|startoftext|>deep learning for image classification and representation<|endoftext|>\n",
            "<|startoftext|>adaptive classification for unstructured text images<|endoftext|>\n",
            "<|startoftext|>deep reinforcement learning and its practical applications to machine learning problems<|endoftext|>\n",
            "<|startoftext|>an empirical study of adversarial training<|endoftext|>\n",
            "<|startoftext|>a hierarchical distributed and flexible probabilistic adversarial training<|endoftext|>\n",
            "<|startoftext|>a comparison of the optimal architectures in high-performance deep learning<|endoftext|>\n",
            "<|startoftext|>a random field regression for a continuous space frame with linear flow<|endoftext|>\n",
            "<|startoftext|>a new batch normalization algorithm for unsupervised learning<|endoftext|>\n",
            "<|startoftext|>a meta-norton learning approach for reinforcement\n",
            "\n",
            "[76 | 124.77] loss=1.89 avg=2.04\n",
            "[77 | 125.66] loss=2.08 avg=2.04\n",
            "[78 | 126.54] loss=1.87 avg=2.04\n",
            "[79 | 127.42] loss=2.10 avg=2.04\n",
            "[80 | 128.30] loss=1.97 avg=2.04\n",
            "[81 | 129.19] loss=2.11 avg=2.04\n",
            "[82 | 130.07] loss=1.90 avg=2.04\n",
            "[83 | 130.95] loss=2.04 avg=2.04\n",
            "[84 | 131.84] loss=1.76 avg=2.03\n",
            "[85 | 132.72] loss=1.95 avg=2.03\n",
            "[86 | 133.60] loss=2.06 avg=2.03\n",
            "[87 | 134.48] loss=2.04 avg=2.03\n",
            "[88 | 135.36] loss=1.89 avg=2.03\n",
            "[89 | 136.24] loss=1.96 avg=2.03\n",
            "[90 | 137.12] loss=1.80 avg=2.02\n",
            "[91 | 138.01] loss=1.88 avg=2.02\n",
            "[92 | 138.89] loss=2.07 avg=2.02\n",
            "[93 | 139.78] loss=1.73 avg=2.02\n",
            "[94 | 140.66] loss=1.78 avg=2.01\n",
            "[95 | 141.54] loss=1.56 avg=2.01\n",
            "[96 | 142.42] loss=2.11 avg=2.01\n",
            "[97 | 143.30] loss=1.93 avg=2.01\n",
            "[98 | 144.18] loss=2.01 avg=2.01\n",
            "[99 | 145.07] loss=1.93 avg=2.01\n",
            "[100 | 145.95] loss=2.02 avg=2.01\n",
            "Saving checkpoint/run2/model-100\n",
            "======== SAMPLE 1 ========\n",
            "<startoftext|>deeplib: a dynamic learning framework for machine learning of non-normative time-series\n",
            "<startoftext|>exact logic gradient<|endoftext|>\n",
            "<startoftext|>multi-threaded kernel-based bayesian sampling for training adversarial neural network<|endoftext|>\n",
            "<startoftext|>expert-guided learning of multi-agent reinforcement learning models on large-scale data<|endoftext|>\n",
            "<startoftext|>improving adversarial machine learning with supervised learning for deep learning applications<|endoftext|>\n",
            "<startoftext|>the optimal gaussian variational inference algorithm for batch normalization in gaussian neural networks<|endoftext|>\n",
            "<startoftext|>a novel ensemble-of-binarized linear-regression model for large-diameter communication networks<|endoftext|>\n",
            "<startoftext|>meta-reinforcement learning with the latent variable function<|endoftext|>\n",
            "<startoftext|>on the robustness of adversarial machine learning models to adversarial test data<|endoftext|>\n",
            "<startoftext|>quantification error minimization in adversarial inference and control systems<|endoftext|>\n",
            "<startoftext|>dictionary learning using the k-means model and b-means<|endoftext|>\n",
            "<startoftext|>learning-out-of-data-learning for automated image retrieval<|endoftext|>\n",
            "<startoftext|>an efficient representation learning algorithm for deep learning<|endoftext|>\n",
            "<startoftext|>learning gaussian distribution normalization based on a model with latent variables<|endoftext|>\n",
            "<startoftext|>learning ganetik: a generalization-proof method to learn nonmanifold stochastic ganets<|endoftext|>\n",
            "<startoftext|>toward automatic stochastic gradient descent for stochastic variational inference<|endoftext|>\n",
            "<startoftext|>a general non-monotonic approximation for estimating joint effects<|endoftext|>\n",
            "<startoftext|>cooperative learning models with k-means<|endoftext|>\n",
            "<startoftext|>exchange-traded markov networks<|endoftext|>\n",
            "<startoftext|>non-linear approximation of bayesian inference by continuous parameters<|endoftext|>\n",
            "<startoftext|>learning-based deep reinforcement learning with latent variables<|endoftext|>\n",
            "<startoftext|>data-oriented deep reinforcement learning based on sparse clustering<|endoftext|>\n",
            "<startoftext|>lstm: latent classification with a linear mixture model<|endoftext|>\n",
            "<startoftext|>learning from multiple records with gaussian mixed models<|endoftext|>\n",
            "<startoftext|>lstmin: sparse multi-agent reinforcement learning by minimizing latent state using linear or nonlinear mixture models<|endoftext|>\n",
            "<startoftext|>structure-preserving deep reinforcement learning via probabilistic multi-agent agents<|endoftext|>\n",
            "<startoftext|>learning probabilistic multiscale inference through gaussian process sampling<|endoftext|>\n",
            "<startoftext|>stochastic differential equations for data-driven regression analysis<|endoftext|>\n",
            "<startoftext|>linear-gradient estimation for graph convolving models<|endoftext|>\n",
            "<startoftext|>generalization bounds for continuous-label variational analysis of k-means<|endoftext|>\n",
            "<startoftext|>optimal algorithm for automatic learning with latent variables by the latent variable-free perspective in stochastic variational inference<|endoftext|>\n",
            "<startoftext|>optimal implementation of non-canonical learning in reinforcement learning<|endoftext|>\n",
            "<startoftext|>meta-anonymization of reinforcement learning via hidden-layer learning of an anonymous target variable<|endoftext|>\n",
            "<startoftext|>a multi-step adversarial training and prediction<|endoftext|>\n",
            "<startoftext|>supermax-clustering with distributed random variables: a fast and efficient method<|endoftext|>\n",
            "<startoftext|>adaptive hyperparameter tuning for high-dimensional multivariate k-means<|endoftext|>\n",
            "<startoftext|>on the effect of clustering on clustering performance<|endoftext|>\n",
            "<startoftext|>optimal training\n",
            "\n",
            "[101 | 170.41] loss=1.83 avg=2.00\n",
            "[102 | 171.29] loss=1.90 avg=2.00\n",
            "[103 | 172.17] loss=1.93 avg=2.00\n",
            "[104 | 173.05] loss=1.90 avg=2.00\n",
            "[105 | 173.94] loss=1.79 avg=2.00\n",
            "[106 | 174.82] loss=1.73 avg=1.99\n",
            "[107 | 175.70] loss=1.89 avg=1.99\n",
            "[108 | 176.58] loss=1.97 avg=1.99\n",
            "[109 | 177.46] loss=1.97 avg=1.99\n",
            "[110 | 178.35] loss=1.99 avg=1.99\n",
            "[111 | 179.24] loss=1.89 avg=1.99\n",
            "[112 | 180.12] loss=1.87 avg=1.99\n",
            "[113 | 181.00] loss=1.99 avg=1.99\n",
            "[114 | 181.88] loss=2.07 avg=1.99\n",
            "[115 | 182.76] loss=1.74 avg=1.98\n",
            "[116 | 183.64] loss=1.81 avg=1.98\n",
            "[117 | 184.53] loss=2.01 avg=1.98\n",
            "[118 | 185.41] loss=2.01 avg=1.98\n",
            "[119 | 186.29] loss=1.92 avg=1.98\n",
            "[120 | 187.18] loss=2.01 avg=1.98\n",
            "[121 | 188.06] loss=1.76 avg=1.98\n",
            "[122 | 188.94] loss=2.11 avg=1.98\n",
            "[123 | 189.82] loss=2.03 avg=1.98\n",
            "[124 | 190.70] loss=2.02 avg=1.98\n",
            "[125 | 191.59] loss=1.94 avg=1.98\n",
            "======== SAMPLE 1 ========\n",
            "olutionolution.org/pulitzer-favors-on-scalable-scalable-pulse-polarimetry--a-critique-of-convergence-predictors with-d-probe-on-dummy: a generalization problem of model selection<|endoftext|>\n",
            "<|startoftext|>a dynamic adversarial training system based on nonlinear neural architecture search<|endoftext|>\n",
            "<|startoftext|>reinforcing features in graph convolutional networks with a novel random forest for reinforcement learning<|endoftext|>\n",
            "<|startoftext|>deep machine learning from spatiotemporal data<|endoftext|>\n",
            "<|startoftext|>a generalized bernoulli process for quantifying time series forecasting<|endoftext|>\n",
            "<|startoftext|>a high order process for deep neural network decomposition<|endoftext|>\n",
            "<|startoftext|>the case of the deep network in the haystack<|endoftext|>\n",
            "<|startoftext|>a novel approach to the characterization of deep neural networks via deep q-learning<|endoftext|>\n",
            "<|startoftext|>a model-free approach to graph reconstruction<|endoftext|>\n",
            "<|startoftext|>a survey of neural networks architectures for high-dimensional data<|endoftext|>\n",
            "<|startoftext|>graph neural networks with arbitrary regularization<|endoftext|>\n",
            "<|startoftext|>neural networks for the control of micro air conditioners<|endoftext|>\n",
            "<|startoftext|>scalable classification of complex neural architectures using graph convolution<|endoftext|>\n",
            "<|startoftext|>detecting the origin of nonparanormally distributed noise in deep dynamical systems<|endoftext|>\n",
            "<|startoftext|>sparsity and prediction performance of deep neural networks over graph convolutional networks using convolutional k-means<|endoftext|>\n",
            "<|startoftext|>an optimal transport for deep generative adversarial network<|endoftext|>\n",
            "<|startoftext|>a simple model of a subspace with linear flow<|endoftext|>\n",
            "<|startoftext|>understanding and improving a single sample analysis framework for multi-label labeling<|endoftext|>\n",
            "<|startoftext|>bayesian prediction error bounds and the limit of bayesian optimization <|endoftext|>\n",
            "<|startoftext|>data reduction via data augmentation<|endoftext|>\n",
            "<|startoftext|>data-driven model optimisation<|endoftext|>\n",
            "<|startoftext|>neural network based hyperparameters for the measurement of oxygenated air temperature<|endoftext|>\n",
            "<|startoftext|>improving and preserving robust data for training of machine learning models using graph convolutional networks<|endoftext|>\n",
            "<|startoftext|>fractional linear models of time series with non-sparsity information<|endoftext|>\n",
            "<|startoftext|>quantifying the effectiveness of gradient descent algorithms by bayesian neural kernel methods<|endoftext|>\n",
            "<|startoftext|>robust and efficient non-parametric feature normalization: the case of deep networks<|endoftext|>\n",
            "<|startoftext|>dynamic regression of time course models using parametric autoencoder with cross entropy<|endoftext|>\n",
            "<|startoftext|>towards a robust prediction of the state of a neural network based on multiple data sets<|endoftext|>\n",
            "<|startoftext|>quantifying the relative impact of adversarial training<|endoftext|>\n",
            "<|startoftext|>a meta-learning approach to the classification of mr-2 models<|endoftext|>\n",
            "<|startoftext|>learning a sparse model by convolutional neural networks in both linear and nonlinear environments<|endoftext|>\n",
            "<|startoftext|>favoring sparse subspaces when performing deep learning on time series and large datasets<|endoftext|>\n",
            "<|startoftext|>a hierarchical graph convolutional language model for classification of large-scale datasets<|endoftext|>\n",
            "<|startoftext|>a simple and efficient algorithm for learning a continuous metric with high dimensional time series<|endoftext|>\n",
            "<|startoftext|>federated learning for high-performance object detection<|endof\n",
            "\n",
            "[126 | 208.68] loss=1.94 avg=1.98\n",
            "[127 | 209.56] loss=2.08 avg=1.98\n",
            "[128 | 210.44] loss=1.97 avg=1.98\n",
            "[129 | 211.33] loss=1.79 avg=1.98\n",
            "[130 | 212.21] loss=1.96 avg=1.98\n",
            "[131 | 213.09] loss=1.80 avg=1.98\n",
            "[132 | 213.97] loss=2.15 avg=1.98\n",
            "[133 | 214.85] loss=2.04 avg=1.98\n",
            "[134 | 215.73] loss=1.74 avg=1.98\n",
            "[135 | 216.61] loss=1.70 avg=1.97\n",
            "[136 | 217.49] loss=1.78 avg=1.97\n",
            "[137 | 218.38] loss=1.84 avg=1.97\n",
            "[138 | 219.26] loss=2.05 avg=1.97\n",
            "[139 | 220.14] loss=1.84 avg=1.97\n",
            "[140 | 221.03] loss=1.84 avg=1.97\n",
            "[141 | 221.91] loss=2.00 avg=1.97\n",
            "[142 | 222.79] loss=2.16 avg=1.97\n",
            "[143 | 223.67] loss=1.98 avg=1.97\n",
            "[144 | 224.56] loss=1.95 avg=1.97\n",
            "[145 | 225.44] loss=1.92 avg=1.97\n",
            "[146 | 226.32] loss=1.50 avg=1.96\n",
            "[147 | 227.21] loss=1.85 avg=1.96\n",
            "[148 | 228.09] loss=1.68 avg=1.96\n",
            "[149 | 228.98] loss=1.93 avg=1.96\n",
            "[150 | 229.86] loss=1.72 avg=1.95\n",
            "======== SAMPLE 1 ========\n",
            " uncertainty>\n",
            "<|startoftext|>on a phylogenetic representation of the brain network using information theory*<|endoftext|>\n",
            "<|startoftext|>on the use of the deep learning framework for data processing and learning from sparse data<|endoftext|>\n",
            "<|startoftext|>a comparative analysis of neural language model selection in a diverse range of languages<|endoftext|>\n",
            "<|startoftext|>on mixed model learning with mixed features<|endoftext|>\n",
            "<|startoftext|>a survey of deep learning architecture search based on hierarchical data representation and its applications in large-scale machine learning<|endoftext|>\n",
            "<|startoftext|>the effects of training in the case of a misspecified deep convolutional model<|endoftext|>\n",
            "<|startoftext|>on the quality and fairness of adversarial evidence through non-supervised training<|endoftext|>\n",
            "<|startoftext|>sparse and rich language models for learning sparse and rich phonemes via an optimal network<|endoftext|>\n",
            "<|startoftext|>learning from data with distributed recurrent neural networks<|endoftext|>\n",
            "<|startoftext|>on the robustness of deep learning models to the use of unknown and complex data<|endoftext|>\n",
            "<|startoftext|>on the role of language perception in sentence formation<|endoftext|>\n",
            "<|startoftext|>on the relationship between sentence structure and sentence structure inference in handwritten language<|endoftext|>\n",
            "<|startoftext|>on the relationship of sentence structure and sentence structure for sentence synthesis<|endoftext|>\n",
            "<|startoftext|>online learning with offline neural network training<|endoftext|>\n",
            "<|startoftext|>the influence of feature learning on attentional control<|endoftext|>\n",
            "<|startoftext|>on the impact of local representations<|endoftext|>\n",
            "<|startoftext|>a model-based framework on the role of localization in the classification of learning disability in children<|endoftext|>\n",
            "<|startoftext|>solving for attention in deep learning<|endoftext|>\n",
            "<|startoftext|>understanding and reducing the influence of false positive expectations in machine learning<|endoftext|>\n",
            "<|startoftext|>understanding the influence of the environment in deep learning<|endoftext|>\n",
            "<|startoftext|>improved image segmentation via latent factor model inference<|endoftext|>\n",
            "<|startoftext|>learning to learn from multiple learning models through nonlinear information flow<|endoftext|>\n",
            "<|startoftext|>on the role of nonstandard training on neural networks<|endoftext|>\n",
            "<|startoftext|>dictionary learning with local features<|endoftext|>\n",
            "<|startoftext|>on the distribution of covariance and standard errors among latent models with neural connections<|endoftext|>\n",
            "<|startoftext|>solving deep neural network models with multiple weights and hidden cnn<|endoftext|>\n",
            "<|startoftext|>comparison of deep learning models using a neural architecture search<|endoftext|>\n",
            "<|startoftext|>adaptive multi-objective stochastic gradient-based optimization with random features<|endoftext|>\n",
            "<|startoftext|>on the role of memory in deep learning for neural network learning<|endoftext|>\n",
            "<|startoftext|>learning for neural architectures<|endoftext|>\n",
            "<|startoftext|>dive in search<|endoftext|>\n",
            "<|startoftext|>on the impact of neural network training on speech perception and comprehension<|endoftext|>\n",
            "<|startoftext|>deep bayesian autoencoders for data-driven analysis of spoken language<|endoftext|>\n",
            "<|startoftext|>on the use of neural networks for graph neural networks<|endoftext|>\n",
            "<|startoftext|>on the effects of the visual and auditory attention mechanisms on the recognition and classification of human speech<|endoftext|>\n",
            "<|startoftext|>dynamic learning of neural networks for speech recognition<|endoftext|>\n",
            "<|startoftext|>towards an understanding of the influence of neural networks on artificial intelligence<|endoftext|>\n",
            "<|startoftext|>on the cost-effectiveness of deep learning in nonlinear programming<|endoftext\n",
            "\n",
            "[151 | 246.81] loss=1.65 avg=1.95\n",
            "[152 | 247.69] loss=1.83 avg=1.95\n",
            "[153 | 248.57] loss=2.13 avg=1.95\n",
            "[154 | 249.45] loss=1.87 avg=1.95\n",
            "[155 | 250.33] loss=1.84 avg=1.95\n",
            "[156 | 251.23] loss=1.83 avg=1.95\n",
            "[157 | 252.11] loss=1.88 avg=1.95\n",
            "[158 | 252.99] loss=1.98 avg=1.95\n",
            "[159 | 253.87] loss=1.79 avg=1.94\n",
            "[160 | 254.76] loss=2.00 avg=1.94\n",
            "[161 | 255.64] loss=2.03 avg=1.95\n",
            "[162 | 256.53] loss=1.88 avg=1.94\n",
            "[163 | 257.41] loss=1.84 avg=1.94\n",
            "[164 | 258.29] loss=1.94 avg=1.94\n",
            "[165 | 259.18] loss=1.97 avg=1.94\n",
            "[166 | 260.06] loss=1.78 avg=1.94\n",
            "[167 | 260.94] loss=1.58 avg=1.94\n",
            "[168 | 261.83] loss=2.02 avg=1.94\n",
            "[169 | 262.71] loss=1.79 avg=1.94\n",
            "[170 | 263.59] loss=2.03 avg=1.94\n",
            "[171 | 264.47] loss=2.23 avg=1.94\n",
            "[172 | 265.35] loss=1.82 avg=1.94\n",
            "[173 | 266.24] loss=1.87 avg=1.94\n",
            "[174 | 267.12] loss=1.83 avg=1.94\n",
            "[175 | 268.01] loss=1.79 avg=1.94\n",
            "======== SAMPLE 1 ========\n",
            " learning dynamic-preserving ensemble method for large-scale learning via a dynamic adaptive process architecture<|endoftext|>\n",
            "<|startoftext|>dynamic learning on small sub-graphs<|endoftext|>\n",
            "<|startoftext|>reinforcement learning in reinforcement learning and its application to the rakunekutta effect<|endoftext|>\n",
            "<|startoftext|>adaptive-unlikelihoods in network analysis and regression<|endoftext|>\n",
            "<|startoftext|>predictor invariant generative models for high-dimensional data<|endoftext|>\n",
            "<|startoftext|>predicting real-world mobile traffic event<|endoftext|>\n",
            "<|startoftext|>lipschitz algorithm for low-rank matrix factorization<|endoftext|>\n",
            "<|startoftext|>a generic framework for graph networks<|endoftext|>\n",
            "<|startoftext|>a generic framework for bayesian optimization in online variational inference<|endoftext|>\n",
            "<|startoftext|>progressive regression with low-dimensional random perturbations<|endoftext|>\n",
            "<|startoftext|>understanding the effect of adversarial examples in deep learning<|endoftext|>\n",
            "<|startoftext|>a cross-entropy based metric learning model for non-stationary multi-objective optimization<|endoftext|>\n",
            "<|startoftext|>learning a sparse distribution with deep learning for data driven reinforcement learning<|endoftext|>\n",
            "<|startoftext|>learning sparse representations of graph elements<|endoftext|>\n",
            "<|startoftext|>exploring the role of adversarial examples in adversarially robust generative modeling<|endoftext|>\n",
            "<|startoftext|>improving multi-label learning by minimizing training time for feature insertion<|endoftext|>\n",
            "<|startoftext|>improving model inference and adversarial robustness<|endoftext|>\n",
            "<|startoftext|>a generalized probabilistic approach to adversarial examples for learning adversarial examples<|endoftext|>\n",
            "<|startoftext|>lipschitz algorithm for multivariate data in bayesian optimization<|endoftext|>\n",
            "<|startoftext|>predicting the state of the art: improving robustness of an algorithm<|endoftext|>\n",
            "<|startoftext|>predicting the time to market price trend of two major financial markets<|endoftext|>\n",
            "<|startoftext|>adversarial attacks on deep reinforcement learning models<|endoftext|>\n",
            "<|startoftext|>neural networks to augment offline representation learning<|endoftext|>\n",
            "<|startoftext|>finite-sample analysis for high-dimensional data<|endoftext|>\n",
            "<|startoftext|>learning hierarchical clustering representations with a generalized model of the neural network<|endoftext|>\n",
            "<|startoftext|>fault tolerance in artificial intelligence<|endoftext|>\n",
            "<|startoftext|>progressive bayesian optimization<|endoftext|>\n",
            "<|startoftext|>learning and validating a neural network for high-dimensional data<|endoftext|>\n",
            "<|startoftext|>mixture learning for online learning algorithms<|endoftext|>\n",
            "<|startoftext|>learning robustness of an artificial intelligence model to adversarial examples<|endoftext|>\n",
            "<|startoftext|>learning optimal policies for large-scale data: a testable intuition of online learning<|endoftext|>\n",
            "<|startoftext|>exploring adversarial examples in deep reinforcement learning<|endoftext|>\n",
            "<|startoftext|>learning adversarial examples in offline inference<|endoftext|>\n",
            "<|startoftext|>deepevolution: dedecapping the noisy domain<|endoftext|>\n",
            "<|startoftext|>learning to manage risk in a multi-agent context<|endoftext|>\n",
            "<|startoftext|>deep learning with random initialization<|endoftext|>\n",
            "<|startoftext|>sparse and large-scale neural networks with adaptive training<|endoftext|>\n",
            "<|startoftext|>distributionally robust neural networks<|endoftext|>\n",
            "<|startoftext|>an experimental study on local maxima in random forest<|endoftext|>\n",
            "<|startoftext|>the problem with the sample size of\n",
            "\n",
            "[176 | 284.91] loss=2.01 avg=1.94\n",
            "[177 | 285.79] loss=1.70 avg=1.93\n",
            "[178 | 286.67] loss=1.99 avg=1.93\n",
            "[179 | 287.56] loss=2.01 avg=1.94\n",
            "[180 | 288.44] loss=1.96 avg=1.94\n",
            "[181 | 289.32] loss=1.85 avg=1.93\n",
            "[182 | 290.20] loss=2.02 avg=1.94\n",
            "[183 | 291.09] loss=1.66 avg=1.93\n",
            "[184 | 291.98] loss=1.52 avg=1.93\n",
            "[185 | 292.86] loss=1.63 avg=1.92\n",
            "[186 | 293.75] loss=1.53 avg=1.92\n",
            "[187 | 294.63] loss=2.00 avg=1.92\n",
            "[188 | 295.51] loss=2.17 avg=1.92\n",
            "[189 | 296.40] loss=1.84 avg=1.92\n",
            "[190 | 297.28] loss=1.65 avg=1.92\n",
            "[191 | 298.16] loss=2.05 avg=1.92\n",
            "[192 | 299.04] loss=1.91 avg=1.92\n",
            "[193 | 299.93] loss=1.63 avg=1.92\n",
            "[194 | 300.81] loss=2.12 avg=1.92\n",
            "[195 | 301.70] loss=1.94 avg=1.92\n",
            "[196 | 302.58] loss=1.61 avg=1.92\n",
            "[197 | 303.47] loss=1.72 avg=1.91\n",
            "[198 | 304.35] loss=2.02 avg=1.92\n",
            "[199 | 305.23] loss=1.85 avg=1.91\n",
            "[200 | 306.12] loss=1.92 avg=1.91\n",
            "Saving checkpoint/run2/model-200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "high-resolution \"free-energy\" and \"quantum-inspired\" neural networks for geo-political risk assessment<|endoftext|>\n",
            "<|startoftext|>asymptotic convergence of deep neural networks in gans<|endoftext|>\n",
            "<|startoftext|>the case for graph neural networks over theoretical neural networks<|endoftext|>\n",
            "<|startoftext|>cognitively-aware nodes: learning to learn and recognize from graph neural networks<|endoftext|>\n",
            "<|startoftext|>targeted acoustic domain adaptation<|endoftext|>\n",
            "<|startoftext|>learning adversarial representations in the context of adversarial reinforcement learning<|endoftext|>\n",
            "<|startoftext|>finite-time and bounded-time optimization in graph neural networks<|endoftext|>\n",
            "<|startoftext|>the disentangled representation for the action space of the rnn-learning-machine<|endoftext|>\n",
            "<|startoftext|>graph-based learning with layers<|endoftext|>\n",
            "<|startoftext|>online learning with multi-view and stacked-view graphs<|endoftext|>\n",
            "<|startoftext|>high-resolution and deep-learning-based signal detection for vehicle tracking in mri<|endoftext|>\n",
            "<|startoftext|>transformation loss for deep neural networks<|endoftext|>\n",
            "<|startoftext|>quantifying and quantifying-wise: evaluating the cost of ambiguity in deep neural networks<|endoftext|>\n",
            "<|startoftext|>partial-order convolutional neural networks for real-time signal monitoring<|endoftext|>\n",
            "<|startoftext|>accurate and fast bayesian computation of dynamic rl in the deep reinforcement learning framework<|endoftext|>\n",
            "<|startoftext|>adversarial examples for the bernstein-posterior method<|endoftext|>\n",
            "<|startoftext|>learning to design deep reinforcement learning models with social and communication-efficient reinforcement learning<|endoftext|>\n",
            "<|startoftext|>environmental pattern analysis for learning local connectivity<|endoftext|>\n",
            "<|startoftext|>learning to update and manage neural networks with integrality to the environment<|endoftext|>\n",
            "<|startoftext|>multi-view and stacked-view graph neural networks for large-scale spatial-temporal data augmentation<|endoftext|>\n",
            "<|startoftext|>on the convergence rate of deep neural networks in the context of the rl-learnings-machine<|endoftext|>\n",
            "<|startoftext|>an evaluation of model complexity and training accuracy for machine learning techniques<|endoftext|>\n",
            "<|startoftext|>learning to understand the \"fake\" images used in machine learning models via segmentation<|endoftext|>\n",
            "<|startoftext|>a super-resolution approach to training neural networks for deep neural networks<|endoftext|>\n",
            "<|startoftext|>continual learning: improving on pruning and deep learning<|endoftext|>\n",
            "<|startoftext|>a multi-view learning approach to generate and visualize deep neural networks<|endoftext|>\n",
            "<|startoftext|>the infinite-head sequence: a new class of graph learning for sequence-to-sequence learning<|endoftext|>\n",
            "<|startoftext|>learning-based autonomous driving using both rene and mixtures<|endoftext|>\n",
            "<|startoftext|>circling the rainbow: a quick and easy roadmap for training deep neural networks<|endoftext|>\n",
            "<|startoftext|>learning to separate the truth from the lie in deep learning<|endoftext|>\n",
            "<|startoftext|>meta-learning for deep learning: a new method for inferring deep structure from hidden data<|endoftext|>\n",
            "<|startoftext|>graph machine learning on graphs: a novel approach for hypergraphs<|endoftext|>\n",
            "<|startoftext|>machine learning for detecting art car marks<|endoftext|>\n",
            "<|startoftext|>non-local linear bandits for graph neural networks<|endoftext|>\n",
            "<|startoftext|>a flexible and efficient approach to machine learning training with control<|endoftext|>\n",
            "<|startoftext|>learning to predict mobile phone failures with deep reinforcement learning<|endoftext|>\n",
            "<|startoftext|>libFC: deep\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "5da8ec2f-35be-45a5-e7f5-2f4fe940b411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "c4b8fca1-df0b-451b-bcad-b79fd7184747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Attempting to use uninitialized value model/h11/attn/c_attn/w\n\t [[{{node model/h11/attn/c_attn/w/read}}]]\n\t [[strided_slice_3/_57]]\n  (1) Failed precondition: Attempting to use uninitialized value model/h11/attn/c_attn/w\n\t [[{{node model/h11/attn/c_attn/w/read}}]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5d868bf0f80e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, sample_dir, return_as_list, truncate, destination_path, sample_delim, prefix, seed, nsamples, batch_size, length, temperature, top_k, top_p, include_prefix)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             out = sess.run(output, feed_dict={\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Attempting to use uninitialized value model/h11/attn/c_attn/w\n\t [[node model/h11/attn/c_attn/w/read (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\t [[strided_slice_3/_57]]\n  (1) Failed precondition: Attempting to use uninitialized value model/h11/attn/c_attn/w\n\t [[node model/h11/attn/c_attn/w/read (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'model/h11/attn/c_attn/w/read':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-ac4972886b2f>\", line 7, in <module>\n    sample_every=25)   # steps is max number of training steps\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 197, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 156, in block\n    a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 132, in attn\n    c = conv1d(x, 'c_attn', n_state*3)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 83, in conv1d\n    w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 1500, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 1243, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 567, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 519, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 933, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 2519, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 1688, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 1872, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 203, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 4239, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "95898950-5b6b-47f8-bf68-26df380610a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "phrase = \"\"\"The rise of fake news has forced everyone with a social media account to become a detective, responsible for determining if a post is real before sharing it. Yet, fake news still slips past our defenses, proliferating on the web, amplified by ignorance and carelessness among real users. Beyond the spread of fear mongering and misinformation, fake news can do real damage [https://www.nbcnews.com/business/business-news/fake-news-can-cause-irreversible-damage-companies-sink-their-stock-n995436] to companies and individuals’ reputations. More than ever, we need better ways to detect fake news to prevent its spread. Some fake news is written by real people and is simply believable fiction, but the proliferation of fake news created with deep learning models can be generated in greater volumes and more rapidly than human texts, intensifying the problem. These natural language generation models have improved dramatically in recent years, making the task of catching them even more difficult, as they generate more believable text than ever. \"\"\"\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              prefix=phrase,\n",
        "              nsamples=3,\n",
        "              batch_size=3,\n",
        "              )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The rise of fake news has forced everyone with a social media account to become a detective, responsible for determining if a post is real before sharing it. Yet, fake news still slips past our defenses, proliferating on the web, amplified by ignorance and carelessness among real users. Beyond the spread of fear mongering and misinformation, fake news can do real damage [https://www.nbcnews.com/business/business-news/fake-news-can-cause-irreversible-damage-companies-sink-their-stock-n995436] to companies and individuals’ reputations. More than ever, we need better ways to detect fake news to prevent its spread. Some fake news is written by real people and is simply believable fiction, but the proliferation of fake news created with deep learning models can be generated in greater volumes and more rapidly than human texts, intensifying the problem. These natural language generation models have improved dramatically in recent years, making the task of catching them even more difficult, as they generate more believable text than ever. ʕoʇoԃ pAins<|endoftext|>\n",
            "<|startoftext|>online continuous submodular maximization<|endoftext|>\n",
            "<|startoftext|>geometric graph convolutional neural networks<|endoftext|>\n",
            "<|startoftext|>learning to optimize with linear function approximation<|endoftext|>\n",
            "<|startoftext|>a data-driven model order reduction\n",
            "====================\n",
            "The rise of fake news has forced everyone with a social media account to become a detective, responsible for determining if a post is real before sharing it. Yet, fake news still slips past our defenses, proliferating on the web, amplified by ignorance and carelessness among real users. Beyond the spread of fear mongering and misinformation, fake news can do real damage [https://www.nbcnews.com/business/business-news/fake-news-can-cause-irreversible-damage-companies-sink-their-stock-n995436] to companies and individuals’ reputations. More than ever, we need better ways to detect fake news to prevent its spread. Some fake news is written by real people and is simply believable fiction, but the proliferation of fake news created with deep learning models can be generated in greater volumes and more rapidly than human texts, intensifying the problem. These natural language generation models have improved dramatically in recent years, making the task of catching them even more difficult, as they generate more believable text than ever.  s f m a r g = ( 0.0368 × 10 −25 ) × 10 −9 ( 0.0368 × 10 −24 ) × 10 ( 0.0368 × 10 −22 ) × 10 ( 0.0368 × 10 −21 ) × 10 ( 0.0368 × 10 −20 ) × 10 ( 0.0368 × 10 −18 ) × 10 ( 0.0368 × 10 −16 ) × 10 ( 0.0368\n",
            "====================\n",
            "The rise of fake news has forced everyone with a social media account to become a detective, responsible for determining if a post is real before sharing it. Yet, fake news still slips past our defenses, proliferating on the web, amplified by ignorance and carelessness among real users. Beyond the spread of fear mongering and misinformation, fake news can do real damage [https://www.nbcnews.com/business/business-news/fake-news-can-cause-irreversible-damage-companies-sink-their-stock-n995436] to companies and individuals’ reputations. More than ever, we need better ways to detect fake news to prevent its spread. Some fake news is written by real people and is simply believable fiction, but the proliferation of fake news created with deep learning models can be generated in greater volumes and more rapidly than human texts, intensifying the problem. These natural language generation models have improved dramatically in recent years, making the task of catching them even more difficult, as they generate more believable text than ever. ˆ\n",
            "<|startoftext|>learning to talk to your product or service curmudgeonly: a unified framework for portfolio management perplexity reduction<|endoftext|>\n",
            "<|startoftext|>a phylogenetic analysis of animal placenames reveal an unusual diversity<|endoftext|>\n",
            "<|startoftext|>combining intelligence and machine learning: towards more accurate animal forecasts<|endoftext|>\n",
            "<|startoftext\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb4EKiyix2rC",
        "colab_type": "code",
        "outputId": "596ae568-7fd4-433a-dcb4-ea1b6aee0ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "phrase = \"\"\"The rise of fake news has forced everyone with a social media account to become a detective, responsible for determining if a post is real before sharing it. Yet, fake news still slips past our defenses, proliferating on the web, amplified by ignorance and carelessness among real users. Beyond the spread of fear mongering and misinformation, fake news can do real damage to companies and individuals’ reputations. More than ever, we need better ways to detect fake news to prevent its spread. Some fake news is written by real people and is simply believable fiction, but the proliferation of fake news created with deep learning models can be generated in greater volumes and more rapidly than human texts, intensifying the problem. These natural language generation models have improved dramatically in recent years, making the task of catching them even more difficult, as they generate more believable text than ever. \"\"\"\n",
        " # None is default\n",
        "text = gpt2.generate(sess,\n",
        "              length=40,\n",
        "              temperature=0.7,\n",
        "              prefix=phrase,\n",
        "              nsamples=1,\n",
        "              batch_size=1,\n",
        "              return_as_list=True\n",
        "             )\n",
        "\n",
        "\n",
        "t = text[0].title()\n",
        "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
        "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Rise Of Fake News Has Forced Everyone With A Social Media Account To Become A Detective, Responsible For Determining If A Post Is Real Before Sharing It. Yet, Fake News Still Slips Past Our Defenses, Proliferating On The Web, Amplified By Ignorance And Carelessness Among Real Users. Beyond The Spread Of Fear Mongering And Misinformation, Fake News Can Do Real Damage To Companies And Individuals’ Reputations. More Than Ever, We Need Better Ways To Detect Fake News To Prevent Its Spread. Some Fake News Is Written By Real People And Is Simply Believable Fiction, But The Proliferation Of Fake News Created With Deep Learning Models Can Be Generated In Greater Volumes And More Rapidly Than Human Texts, Intensifying The Problem. These Natural Language Generation Models Have Improved Dramatically In Recent Years, Making The Task Of Catching Them Even More Difficult, As They Generate More Believable Text Than Ever. _________________________________Deep Learning For Automated Icd Coding: A Review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFhH4sVXz-me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "new_phrase = \"\"\"The rise of fake news has forced everyone with a social media account to become a detective, responsible for determining if a post is real before sharing it.\"\"\"\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=40,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      prefix=new_phrase,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "Credit to Max Woolf: \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read Max Woolf's [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information.\n",
        "\n",
        "\n",
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bhs-oqpjnei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}